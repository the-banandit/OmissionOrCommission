{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pretty code\n",
    "%load_ext nb_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange, tqdm\n",
    "from matplotlib.pyplot import figure\n",
    "from newsrelations.query_db.relation_query import DBQueryHandlerCoocc\n",
    "from newsrelations.helper_classes.synonym_handler import SynonymHandler\n",
    "from newsrelations.metrics.distances import DistanceMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_contingency_table_from_single_topic(\n",
    "    relation_models_path, relation_models, topic_of_interest, no_entities=10\n",
    "):\n",
    "    \"\"\"\n",
    "    This function builds a contingency table from a input list of relation models generated with relation_miner.py \n",
    "    in regards to predetermined topic [TOPIC_OF_INTEREST].\n",
    "    The first model in [RELATION_MODELS] is the reference model all other models will be compared with.\n",
    "    The function extracts the top [NO_ENTITIES] co_occuring entities from the model and builds a contingency table.\n",
    "    \n",
    "    input:  relation_models = list\n",
    "            relation_models_path = str \n",
    "            topic_of_interest = str           \n",
    "            no_entities = int (standard 10)\n",
    "            \n",
    "    output: contingency_table = pandas DataFrame [rows = different models, columns = entities]\n",
    "    \"\"\"\n",
    "    E1_SYNSET = 0\n",
    "    E2_SYNSET = 1\n",
    "\n",
    "    # initialize DistanceMeasure with reference-model\n",
    "    dm = DistanceMeasure(relation_models_path, str(relation_models[0]))\n",
    "\n",
    "    # extract top NO_ENTITIES entities\n",
    "    top = dm.get_top_co_occurrences(\n",
    "        topic_of_interest,\n",
    "        cutoff=no_entities,\n",
    "        e1_is_synset=E1_SYNSET,\n",
    "        e2_is_synset=E2_SYNSET,\n",
    "    )\n",
    "    # write first row of contingency_table\n",
    "    contingency_table = pd.DataFrame(\n",
    "        np.array([t[1] for t in top]),\n",
    "        index=[t[0] for t in top],\n",
    "        columns=[str(relation_models[0])],\n",
    "    )\n",
    "\n",
    "    # loop through all remaining models\n",
    "    for model in relation_models[1:]:\n",
    "        # initialize db_handler()\n",
    "        db_handler = DBQueryHandlerCoocc(relation_models_path, model)\n",
    "\n",
    "        # buffer for cooccurrences\n",
    "        co_occs = []\n",
    "        # loop through all entities and get number of co-occurrences\n",
    "        for row in contingency_table.index:\n",
    "            co_occs.append(\n",
    "                len(\n",
    "                    list(\n",
    "                        db_handler.select_relations(\n",
    "                            e1=topic_of_interest.lower(),\n",
    "                            e2=row.lower(),\n",
    "                            e1_is_synset=E1_SYNSET,\n",
    "                            e2_is_synset=E2_SYNSET,\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        contingency_table[str(model)] = co_occs\n",
    "\n",
    "    # transpose the contingency table to get it into the right format\n",
    "    contingency_table = contingency_table.transpose()\n",
    "    return contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_models(relation_models):\n",
    "    \"\"\"\n",
    "    This function lists the models from the relation_models list and their counter index\n",
    "    \n",
    "    Input: relation_models = list\n",
    "    \n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    for model in relation_models:\n",
    "        print(\"model(\" + str(i) + \"): \" + str(model))\n",
    "        i += 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_contingency_table_from_topic_list(\n",
    "    relation_models_path, relation_models, topic_of_interest_list, no_entities=10\n",
    "):\n",
    "    \"\"\"\n",
    "    This function builds a contingency table from a input list of relation models generated with relation_miner.py \n",
    "    in regards to predetermined topic list [topic_of_interest_list].\n",
    "    The first model in [relation_models] is the reference model all other models will be compared with.\n",
    "    The function extracts the top [no_entities] co-occuring entities for the first [topic_of_interest] from the \n",
    "    model and builds a contingency table.\n",
    "    \n",
    "    \n",
    "    input:  relation_models = list\n",
    "            relation_models_path = str \n",
    "            topic_of_interest_list = list           \n",
    "            no_entities = int (standard 10)\n",
    "            \n",
    "    output: contingency_table = pandas DataFrame [rows = different models, columns = entities]\n",
    "    \"\"\"\n",
    "    # identifier for models in relation_models_list\n",
    "    i = 0\n",
    "    # print models with idx\n",
    "    explain_models(relation_models)\n",
    "\n",
    "    # initialize DistanceMeasure with reference-model\n",
    "    dm = DistanceMeasure(relation_models_path, str(relation_models[0]))\n",
    "\n",
    "    # extract top NO_ENTITIES entities\n",
    "    top = dm.get_top_co_occurrences(\n",
    "        topic_of_interest_list[0], cutoff=no_entities, e1_is_synset=0, e2_is_synset=0\n",
    "    )\n",
    "    # write first row of contingency_table\n",
    "    contingency_table = pd.DataFrame(\n",
    "        np.array([t[1] for t in top]),\n",
    "        index=[t[0] for t in top],\n",
    "        columns=[str(topic_of_interest_list[0]) + \" (\" + str(i) + \")\"],\n",
    "    )\n",
    "\n",
    "    # loop through the models\n",
    "    for model in relation_models[:]:\n",
    "        # initialize db_handler()\n",
    "        db_handler = DBQueryHandlerCoocc(relation_models_path, model)\n",
    "\n",
    "        for topic in topic_of_interest_list:\n",
    "            # buffer for co-occurrencces\n",
    "            co_occs = []\n",
    "\n",
    "            # loop through all all entities and get number of co-occurrences\n",
    "            for row in contingency_table.index:\n",
    "                co_occs.append(\n",
    "                    len(\n",
    "                        list(\n",
    "                            db_handler.select_relations(\n",
    "                                e1=topic.lower(),\n",
    "                                e2=row.lower(),\n",
    "                                e1_is_synset=0,\n",
    "                                e2_is_synset=0,\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            contingency_table[str(topic) + \" (\" + str(i) + \")\"] = co_occs\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # transpose the contingency table to get it into the right format\n",
    "    contingency_table = contingency_table.transpose()\n",
    "\n",
    "    return contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_entity_lists(\n",
    "    relation_models_path, relation_models, topic_list1, topic_list2\n",
    "):\n",
    "    \"\"\"\n",
    "    This function creates a contingency table from a two input lists of entities [topic_list1][topic_list2]\n",
    "    from a list of relation models \n",
    "    \n",
    "    input:  relation_models = list\n",
    "            relation_models_path = str \n",
    "            topic_list1 = list           \n",
    "            topic_list2 = list\n",
    "            \n",
    "    output: contingency_table = pandas DataFrame [rows = different models, columns = entities]\n",
    "    \"\"\"\n",
    "    # idx for models\n",
    "    i = 0\n",
    "\n",
    "    # print models with idx\n",
    "    explain_models(relation_models)\n",
    "\n",
    "    # initialize contingency_table\n",
    "    contingency_table = pd.DataFrame(index=topic_list2)\n",
    "\n",
    "    # loop through the models\n",
    "    for model in relation_models:\n",
    "        # initialize db_handler()\n",
    "        db_handler = DBQueryHandlerCoocc(relation_models_path, model)\n",
    "\n",
    "        for topic1 in topic_list1:\n",
    "            # buffer for co-occurrencces\n",
    "            co_occs = []\n",
    "\n",
    "            # loop through all all entities and get number of co-occurrences\n",
    "            for topic2 in topic_list2:\n",
    "                co_occs.append(\n",
    "                    len(\n",
    "                        list(\n",
    "                            db_handler.select_relations(\n",
    "                                e1=topic1.lower(),\n",
    "                                e2=topic2.lower(),\n",
    "                                e1_is_synset=0,\n",
    "                                e2_is_synset=0,\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            contingency_table[str(topic1) + \" (\" + str(i) + \")\"] = co_occs\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # transpose the contingency table to get it into the right format\n",
    "    contingency_table = contingency_table.transpose()\n",
    "\n",
    "    return contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared(contingency_table, print_orig = False, print_expect = False, print_chi_contr = False):\n",
    "    \"\"\"\n",
    "    This function conducts a chi-squared test of independence between the different rows of a contingency table\n",
    "    \n",
    "    input: contingency_table\n",
    "    \n",
    "    ouput: None\n",
    "    \"\"\"\n",
    "    contingency_table = sm.stats.Table(contingency_table)\n",
    "    results = contingency_table.test_nominal_association()\n",
    "    \n",
    "    \n",
    "    # orig contingency table\n",
    "    if print_orig == True:\n",
    "        print(\"Original contingency table:\")\n",
    "        print(contingency_table.table_orig)\n",
    "    # expected values\n",
    "    if print_expect == True:\n",
    "        print(\"\\nExpected values:\")\n",
    "        print(contingency_table.fittedvalues)\n",
    "    # chi-squared contributions\n",
    "    if print_chi_contr == True:\n",
    "        print(\"\\nChi-square contributions:\")\n",
    "        print(contingency_table.chi2_contribs)\n",
    "    \n",
    "    # results\n",
    "    print(\"\\nResults:\")\n",
    "    print(results)\n",
    "\n",
    "   \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_chi_squared_comparison(\n",
    "    relation_models_path, relation_models, topic_of_interest, no_entities\n",
    "):\n",
    "    \"\"\"\n",
    "    This function extracts chi-squared test results for all combinations of news-outlets\n",
    "    \n",
    "    input: contingency_table\n",
    "    \n",
    "    ouput: None\n",
    "    \"\"\"\n",
    "\n",
    "    # extracting a contingency table from a single reference entity\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    df_results = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(relation_models)):\n",
    "        for j in range(len(relation_models)):\n",
    "            models = []\n",
    "            models = [relation_models[i]] + [relation_models[j]]\n",
    "\n",
    "            contingency_table = build_contingency_table_from_single_topic(\n",
    "                relation_models_path, models, topic_of_interest, no_entities\n",
    "            )\n",
    "\n",
    "            contingency_table = sm.stats.Table(contingency_table)\n",
    "            results = contingency_table.test_nominal_association()\n",
    "\n",
    "            df_results[\n",
    "                str(relation_models[i][-10:-7])\n",
    "                + \" - \"\n",
    "                + str(relation_models[j][-10:-7])\n",
    "            ] = [\n",
    "                results.statistic,\n",
    "                results.pvalue,\n",
    "            ]\n",
    "\n",
    "    df_results = df_results.transpose()\n",
    "    df_results = df_results.rename(columns={0: \"chi_sq\", 1: \"p_value\"})\n",
    "    df_results\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables\n",
    "\n",
    "Variable setups for different runs of models (all scraped from commoncrawl.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Try\n",
    "# general tryout on the NewsRelations Library\n",
    "\n",
    "# directory path of relation models\n",
    "RELATION_MODELS_PATH = \"/home/jonas/Documents/GitHub/MasterThesis/models/firstTry\"\n",
    "\n",
    "# model name\n",
    "RELATION_MODELS = [\n",
    "    \"model.sqlite\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Try\n",
    "# models from newssources with different biases \n",
    "#\n",
    "# -year:    2008 (for foxnews 2o08-2010)\n",
    "# -domain:  politics\n",
    "# -sources: NYT & foxnews \n",
    "\n",
    "# directory path of relation models \n",
    "RELATION_MODELS_PATH = \"/home/jonas/Documents/GitHub/MasterThesis/models/secondTry\"\n",
    "\n",
    "# model nameslf, path_or_b\n",
    "RELATION_MODELS = [\n",
    "    \"RMadvanced_2008_politics_nytimes.sqlite\",\n",
    "    \"RM_2009_politics_nytimes.sqlite\",\n",
    "    \"RM_2008-2010_politics_foxnews.sqlite\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third Try\n",
    "# models from newssources with different biases \n",
    "#\n",
    "# -year:    2011\n",
    "# -domain:  news\n",
    "# -sources: reuters & national public radio\n",
    "\n",
    "# directory path of relation models \n",
    "RELATION_MODELS_PATH = \"/home/jonas/Documents/GitHub/MasterThesis/models/thirdTry\"\n",
    "\n",
    "# model names\n",
    "RELATION_MODELS = [\n",
    "    \"RM_2011_news_reuters.sqlite\",\n",
    "    \"RM_2011_news_npr.sqlite\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth try\n",
    "# two models from newssources with different biases each, for comparing slant coherence within\n",
    "# different directions. Timeslots with gapless news reporting were chosen.\n",
    "#\n",
    "# -year:    2011-01-01 - 2011-03-31\n",
    "# -domain:  news\n",
    "# -sources(left):    New York Times (NYT) 637, Washington Post (WP) 508\n",
    "#         (center):  National Public Radio (NPR) 109, Reuters (RET) 300\n",
    "#         (right):   FoxNews (FN) 2735, Newsmax (NM) 180\n",
    "\n",
    "# directory path of relation models\n",
    "RELATION_MODELS_PATH = \"/home/jonas/Documents/GitHub/MasterThesis/models/fourthTry\"\n",
    "\n",
    "# model names\n",
    "RELATION_MODELS = [\n",
    "    \"RM_2011_news_NYT.sqlite\",\n",
    "    \"RM_2011_news_WP.sqlite\",\n",
    "    \"RM_2011_news_NPR.sqlite\",\n",
    "    \"RM_2011_news_RET.sqlite\",\n",
    "    \"RM_2011_news_FN.sqlite\",\n",
    "    \"RM_2011_news_NM.sqlite\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final datasets\n",
    "\n",
    "Final datasets used in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for parameter estimation\n",
    "# for the estiation of parameters number of entities and topic_of_interest\n",
    "#\n",
    "# -year:    2012\n",
    "# -domain:  news\n",
    "# -sources(left):    Huffington Post (HFP) 4909, New York Times (NYT) 2541,\n",
    "#         (center):  CNN (CNN) 2491, Reuters (RET) 2135\n",
    "#         (right):   FoxNews (FXN) 3784, (WSJ) 1215\n",
    "\n",
    "# directory path of relation models\n",
    "RELATION_MODELS_PATH = \"/home/jonas/Documents/GitHub/MasterThesis/models/calibrationDataset\"\n",
    "\n",
    "# model names\n",
    "RELATION_MODELS = [\n",
    "    \"RM_2012_news_HFP.sqlite\",\n",
    "    \"RM_2012_news_NYT.sqlite\",\n",
    "    \"RM_2012_news_CNN.sqlite\",\n",
    "    \"RM_2012_news_RET.sqlite\",\n",
    "    \"RM_2012_news_FXN.sqlite\",\n",
    "    \"RM_2012_news_WSJ.sqlite\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Dataset\n",
    "# final daset based on works of Budak (2016) Flaxmann (2016) and Groseclose (2015)\n",
    "#\n",
    "# -year:    2011\n",
    "# -domain:  news\n",
    "# -sources(left):    Huffington Post (HFP) 14876, LA Times (LAT) 445, New York Times (NYT) 11281,\n",
    "#                    Washington Post (WP) 14814, Daily KOS (DKO) 123\n",
    "#         (center):  BBC (BBC) 52, CNN (CNN) 2652, Reuters (RET) 16767, Yahoo News (YHN) 211\n",
    "#         (right):   Chicago Tribune (CTB) 2843, FoxNews (FXN) 6508, NBC (NBC) 3958, USA Today (UST) 171\n",
    "#                    Wall Street Journal (WSJ) 2522, Breitbart (BBT) 76\n",
    "\n",
    "# directory path of relation models\n",
    "RELATION_MODELS_PATH = \"/home/jonas/Documents/GitHub/MasterThesis/models/finalDataset\"\n",
    "\n",
    "# model names\n",
    "RELATION_MODELS = [\n",
    "    \"RM_2011_news_BBC.sqlite\",\n",
    "    \"RM_2011_news_BBT.sqlite\",\n",
    "    \"RM_2011_news_CNN.sqlite\",\n",
    "    \"RM_2011_news_CTB.sqlite\",\n",
    "    \"RM_2011_news_DKO.sqlite\",\n",
    "    \"RM_2011_news_FXN.sqlite\",\n",
    "    \"RM_2011_news_HFP.sqlite\",\n",
    "    \"RM_2011_news_LAT.sqlite\",\n",
    "    \"RM_2011_news_NBC.sqlite\",\n",
    "    \"RM_2011_news_NYT.sqlite\",\n",
    "    \"RM_2011_news_RET.sqlite\",\n",
    "    \"RM_2011_news_UST.sqlite\",\n",
    "    \"RM_2011_news_WPO.sqlite\",\n",
    "    \"RM_2011_news_WSJ.sqlite\",\n",
    "    \"RM_2011_news_YHN.sqlite\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "general global variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to results folder\n",
    "RESULTS_PATH = \"/home/jonas/Documents/GitHub/MasterThesis/results\"\n",
    "\n",
    "# number of entities you want to compare\n",
    "NO_ENTITIES = 8\n",
    "# number of entities in test-loops\n",
    "N = 41\n",
    "\n",
    "# reference entity\n",
    "TOPIC_OF_INTEREST = \"united_states\"\n",
    "\n",
    "# reference entity list\n",
    "TOPIC_OF_INTEREST_LIST2 = [\n",
    "    \"united_states\",\n",
    "    \"germany\",\n",
    "    \"russia\",\n",
    "    \"india\",\n",
    "    \"china\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter estimation\n",
    "\n",
    "## Estimation of \"n\" - number of entities \n",
    "\n",
    "To estimate the optimal n, I examine the stability of the p-value during chi-square tests with different n's, within same slant and between different slant groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "# directory path of relation models\n",
    "RELATION_MODELS_PATH = \"/home/jonas/Documents/GitHub/MasterThesis/models/calibrationDataset\"\n",
    "\n",
    "# model names\n",
    "RELATION_MODELS = [\n",
    "    \"RM_2012_news_HFP.sqlite\",\n",
    "    \"RM_2012_news_NYT.sqlite\",\n",
    "    \"RM_2012_news_CNN.sqlite\",\n",
    "    \"RM_2012_news_RET.sqlite\",\n",
    "    \"RM_2012_news_FXN.sqlite\",\n",
    "    \"RM_2012_news_WSJ.sqlite\",\n",
    "]\n",
    "\n",
    "# path to results folder\n",
    "RESULTS_PATH = \"/home/jonas/Documents/GitHub/MasterThesis/results/\"\n",
    "\n",
    "# max number of entities + 1, to compare with the reference entity\n",
    "NO_ENTITIES = 41 \n",
    "\n",
    "# reference entity - \n",
    "TOPIC_OF_INTEREST = \"united_states\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# estimate optimal n - within same slant groups\n",
    "models = [\n",
    "    [RELATION_MODELS[0], RELATION_MODELS[1]],\n",
    "    [RELATION_MODELS[1], RELATION_MODELS[0]],\n",
    "    [RELATION_MODELS[2], RELATION_MODELS[3]],\n",
    "    [RELATION_MODELS[3], RELATION_MODELS[2]],\n",
    "    [RELATION_MODELS[4], RELATION_MODELS[5]],\n",
    "    [RELATION_MODELS[5], RELATION_MODELS[4]],\n",
    "]\n",
    "\n",
    "# initialize dataframes and counter for columns\n",
    "df = pd.DataFrame()\n",
    "df_labels = pd.DataFrame()\n",
    "i = 0\n",
    "\n",
    "# loop through model constellation in models list\n",
    "for constellation in tqdm(models):\n",
    "    i += 1\n",
    "    df_results = pd.DataFrame()\n",
    "    df_singleLabels = pd.DataFrame()\n",
    "\n",
    "    # loop through entity numbers until max entity is reached\n",
    "    for n in range(1, NO_ENTITIES):\n",
    "        # create SQL query and build contingency table for sm.stats\n",
    "        contingency_table = build_contingency_table_from_single_topic(\n",
    "            RELATION_MODELS_PATH, constellation, TOPIC_OF_INTEREST, n\n",
    "        )\n",
    "        contingency_table = sm.stats.Table(contingency_table)\n",
    "        # calculate results + add them to dataframe\n",
    "        results = contingency_table.test_nominal_association()\n",
    "        df_results[n] = [results.statistic, results.pvalue]\n",
    "\n",
    "    df_results = df_results.transpose()\n",
    "    df_results = df_results.rename(\n",
    "        columns={0: \"chi_sq\" + str(i), 1: \"p_value\" + str(i)}\n",
    "    )\n",
    "    \"\"\"\n",
    "    # initialize path for backup save of dataframe\n",
    "    path = (\n",
    "        \"/home/jonas/Documents/GitHub/MasterThesis/results/nSameSlant_\"\n",
    "        + str(constellation[0][-10:-7])\n",
    "        + \"_\"\n",
    "        + str(constellation[1][-10:-7])\n",
    "        + \".csv\"\n",
    "    )\n",
    "    \"\"\"\n",
    "    # labels for visualization\n",
    "    df_singleLabels = [\n",
    "        str(str(constellation[0][-10:-7]) + \"-\" + str(constellation[1][-10:-7]))\n",
    "    ]\n",
    "    df_labels = df_labels.append(df_singleLabels, ignore_index=True)\n",
    "\n",
    "    # save results to csv\n",
    "    # df_results.to_csv(path)\n",
    "    # concat all results to one big dataframe\n",
    "    df = pd.concat([df, df_results], axis=1)\n",
    "\n",
    "# save results to csv\n",
    "df.to_csv(\"/home/jonas/Documents/GitHub/MasterThesis/results/nSameSlant_All.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualze\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "df.plot(kind=\"line\", y=\"p_value1\", color=\"red\", ax=ax, label=df_labels[0][0])\n",
    "df.plot(kind=\"line\", y=\"p_value2\", color=\"orange\", ax=ax, label=df_labels[0][1])\n",
    "df.plot(kind=\"line\", y=\"p_value3\", color=\"green\", ax=ax, label=df_labels[0][2])\n",
    "df.plot(kind=\"line\", y=\"p_value4\", color=\"yellow\", ax=ax, label=df_labels[0][3])\n",
    "df.plot(kind=\"line\", y=\"p_value5\", color=\"blue\", ax=ax, label=df_labels[0][4])\n",
    "df.plot(kind=\"line\", y=\"p_value6\", color=\"blue\", ax=ax, label=df_labels[0][5])\n",
    "\n",
    "\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.title(\n",
    "    \"Behavior of p-values in respective to number of examined entities\\n within same slant groups\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate optimal n - within different slant groups\n",
    "models = [\n",
    "    [RELATION_MODELS[0], RELATION_MODELS[2]],\n",
    "    [RELATION_MODELS[0], RELATION_MODELS[3]],\n",
    "    [RELATION_MODELS[0], RELATION_MODELS[4]],\n",
    "    [RELATION_MODELS[0], RELATION_MODELS[5]],\n",
    "    [RELATION_MODELS[1], RELATION_MODELS[2]],\n",
    "    [RELATION_MODELS[1], RELATION_MODELS[3]],\n",
    "    [RELATION_MODELS[1], RELATION_MODELS[4]],\n",
    "    [RELATION_MODELS[1], RELATION_MODELS[5]],\n",
    "    [RELATION_MODELS[2], RELATION_MODELS[0]],\n",
    "    [RELATION_MODELS[2], RELATION_MODELS[1]],\n",
    "    [RELATION_MODELS[2], RELATION_MODELS[4]],\n",
    "    [RELATION_MODELS[2], RELATION_MODELS[5]],\n",
    "    [RELATION_MODELS[3], RELATION_MODELS[0]],\n",
    "    [RELATION_MODELS[3], RELATION_MODELS[1]],\n",
    "    [RELATION_MODELS[3], RELATION_MODELS[4]],\n",
    "    [RELATION_MODELS[3], RELATION_MODELS[5]],\n",
    "    [RELATION_MODELS[4], RELATION_MODELS[0]],\n",
    "    [RELATION_MODELS[4], RELATION_MODELS[1]],\n",
    "    [RELATION_MODELS[4], RELATION_MODELS[2]],\n",
    "    [RELATION_MODELS[4], RELATION_MODELS[3]],\n",
    "    [RELATION_MODELS[5], RELATION_MODELS[0]],\n",
    "    [RELATION_MODELS[5], RELATION_MODELS[1]],\n",
    "    [RELATION_MODELS[5], RELATION_MODELS[2]],\n",
    "    [RELATION_MODELS[5], RELATION_MODELS[3]],\n",
    "]\n",
    "\n",
    "# initialize dataframes and counter for column name\n",
    "df = pd.DataFrame()\n",
    "df_labels = pd.DataFrame()\n",
    "i = 0\n",
    "\n",
    "# loop through model constellation in models list\n",
    "for constellation in tqdm(models):\n",
    "    i += 1\n",
    "    df_results = pd.DataFrame()\n",
    "    df_singleLabels = pd.DataFrame()\n",
    "\n",
    "    # loop through entity numbers until max entity is reached\n",
    "    for n in range(1, NO_ENTITIES):\n",
    "        # create SQL query and build contingency table for sm.stats\n",
    "        contingency_table = build_contingency_table_from_single_topic(\n",
    "            RELATION_MODELS_PATH, constellation, TOPIC_OF_INTEREST, n\n",
    "        )\n",
    "        contingency_table = sm.stats.Table(contingency_table)\n",
    "        # calculate results + add them to dataframe\n",
    "        results = contingency_table.test_nominal_association()\n",
    "        df_results[n] = [results.statistic, results.pvalue]\n",
    "\n",
    "    df_results = df_results.transpose()\n",
    "    df_results = df_results.rename(\n",
    "        columns={0: \"chi_sq\" + str(i), 1: \"p_value\" + str(i)}\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    # initialize path for backup save of dataframe\n",
    "    path = (\n",
    "        \"/home/jonas/Documents/GitHub/MasterThesis/results/nDiffSlant_\"\n",
    "        + str(constellation[0][-10:-7])\n",
    "        + \"_\"\n",
    "        + str(constellation[1][-10:-7])\n",
    "        + \".csv\"\n",
    "    )\n",
    "    \"\"\"\n",
    "    # labels for visualization\n",
    "    df_singleLabels = [\n",
    "        str(str(constellation[0][-10:-7]) + \"-\" + str(constellation[1][-10:-7]))\n",
    "    ]\n",
    "    df_labels = df_labels.append(df_singleLabels, ignore_index=True)\n",
    "\n",
    "    # save results to csv\n",
    "    # df_results.to_csv(path)\n",
    "    # concat all results to one big dataframe\n",
    "    df = pd.concat([df, df_results], axis=1)\n",
    "\n",
    "# save results to csv\n",
    "df.to_csv(\"/home/jonas/Documents/GitHub/MasterThesis/results/nDiffSlant_All.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualze\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "df.plot(kind=\"line\", y=\"p_value1\", color=\"red\", ax=ax, label=df_labels[0][0])\n",
    "df.plot(kind=\"line\", y=\"p_value2\", color=\"orange\", ax=ax, label=df_labels[0][1])\n",
    "df.plot(kind=\"line\", y=\"p_value3\", color=\"green\", ax=ax, label=df_labels[0][2])\n",
    "df.plot(kind=\"line\", y=\"p_value4\", color=\"blue\", ax=ax, label=df_labels[0][3])\n",
    "df.plot(kind=\"line\", y=\"p_value5\", color=\"yellow\", ax=ax, label=df_labels[0][4])\n",
    "df.plot(kind=\"line\", y=\"p_value6\", color=\"blue\", ax=ax, label=df_labels[0][5])\n",
    "df.plot(kind=\"line\", y=\"p_value7\", color=\"red\", ax=ax, label=df_labels[0][6])\n",
    "df.plot(kind=\"line\", y=\"p_value8\", color=\"orange\", ax=ax, label=df_labels[0][7])\n",
    "df.plot(kind=\"line\", y=\"p_value9\", color=\"green\", ax=ax, label=df_labels[0][8])\n",
    "df.plot(kind=\"line\", y=\"p_value10\", color=\"blue\", ax=ax, label=df_labels[0][9])\n",
    "df.plot(kind=\"line\", y=\"p_value11\", color=\"yellow\", ax=ax, label=df_labels[0][10])\n",
    "df.plot(kind=\"line\", y=\"p_value12\", color=\"blue\", ax=ax, label=df_labels[0][11])\n",
    "df.plot(kind=\"line\", y=\"p_value13\", color=\"red\", ax=ax, label=df_labels[0][12])\n",
    "df.plot(kind=\"line\", y=\"p_value14\", color=\"red\", ax=ax, label=df_labels[0][13])\n",
    "df.plot(kind=\"line\", y=\"p_value15\", color=\"red\", ax=ax, label=df_labels[0][14])\n",
    "df.plot(kind=\"line\", y=\"p_value16\", color=\"red\", ax=ax, label=df_labels[0][15])\n",
    "df.plot(kind=\"line\", y=\"p_value17\", color=\"red\", ax=ax, label=df_labels[0][16])\n",
    "df.plot(kind=\"line\", y=\"p_value18\", color=\"red\", ax=ax, label=df_labels[0][17])\n",
    "df.plot(kind=\"line\", y=\"p_value19\", color=\"red\", ax=ax, label=df_labels[0][18])\n",
    "df.plot(kind=\"line\", y=\"p_value20\", color=\"red\", ax=ax, label=df_labels[0][19])\n",
    "df.plot(kind=\"line\", y=\"p_value21\", color=\"red\", ax=ax, label=df_labels[0][20])\n",
    "df.plot(kind=\"line\", y=\"p_value22\", color=\"red\", ax=ax, label=df_labels[0][21])\n",
    "df.plot(kind=\"line\", y=\"p_value23\", color=\"red\", ax=ax, label=df_labels[0][22])\n",
    "df.plot(kind=\"line\", y=\"p_value24\", color=\"red\", ax=ax, label=df_labels[0][23])\n",
    "\n",
    "\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.title(\n",
    "    \"Behavior of p-values in respective to number of examined entities \\nbetween different slant groups\"\n",
    ")\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of optimal number of \"topic_of_interest\"\n",
    "\n",
    "To estimate the optimal number of different reference-entities, I examine the behavior of the p-value witin and between different slant groups, under varying reference entities. The entities are derived from XXXX (XXXX) and represent controverse topics and non-controvers topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "# directory path of relation models\n",
    "RELATION_MODELS_PATH = \"/home/jonas/Documents/GitHub/MasterThesis/models/calibrationDataset\"\n",
    "\n",
    "# model names\n",
    "RELATION_MODELS = [\n",
    "    \"RM_2012_news_HFP.sqlite\",\n",
    "    \"RM_2012_news_NYT.sqlite\",\n",
    "    \"RM_2012_news_CNN.sqlite\",\n",
    "    \"RM_2012_news_RET.sqlite\",\n",
    "    \"RM_2012_news_FXN.sqlite\",\n",
    "    \"RM_2012_news_WSJ.sqlite\",\n",
    "]\n",
    "\n",
    "# path to results folder\n",
    "RESULTS_PATH = \"/home/jonas/Documents/GitHub/MasterThesis/results/\"\n",
    "\n",
    "# number of entities to compare with the reference entitiy\n",
    "NO_ENTITIES = 41 \n",
    "\n",
    "# reference entities \n",
    "TOPIC_OF_INTEREST_LIST = [\n",
    "    \"white_house\",\n",
    "    \"obama\",\n",
    "    \"bush\",\n",
    "    \"NRA\",\n",
    "    \"white_house\",\n",
    "    \"united_states\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis testing \n",
    "\n",
    "## Hypothesis 1\n",
    "\n",
    "1.1 Within same slant groups the co-occurring entities are independent from the\n",
    "    news-outlet\n",
    "\n",
    "1.2 Between different slant groups the co-occurring entities are dependent from\n",
    "    the news-outlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare everything with everything\n",
    "\n",
    "df = do_chi_squared_comparison(\n",
    "    RELATION_MODELS_PATH, RELATION_MODELS, TOPIC_OF_INTEREST, NO_ENTITIES\n",
    ")\n",
    "\n",
    "# save to csv\n",
    "df.to_csv(\"/home/jonas/Documents/GitHub/MasterThesis/results/hyp1.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
