{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Thesis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook can be understood as a walkthrough through the experiments I ran for my master thesis.\n",
    "To keep it somewhat clear I pushed large functions beyond the imports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [Imports & functions](#imports&functions)\n",
    "### 2. [Tryout-datasets](#datasets)\n",
    "### 3. [Final datasets](#finalDatasets)\n",
    "### 4. [Parameter tuning](#parameterTuning)\n",
    "#### 4.1 [n - entity estimation](#nEntity)\n",
    "#### 4.2 [reference entity estimation](#refEntity)\n",
    "### 5. [Hypothesis testing](#hypothesisTesting)\n",
    "#### 5.1 [Hypothesis 1](#hyp1)\n",
    "#### 5.2 [Hypothesis 2](#hyp2)\n",
    "#### 5.3 [Hypothesis 3](#hyp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and functions <a class=\"anchor\" id=\"imports&functions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pretty code\n",
    "%load_ext nb_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import nltk.data\n",
    "import statistics\n",
    "import networkx as nx\n",
    "import math\n",
    "import warnings\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange, tqdm\n",
    "from matplotlib.pyplot import figure\n",
    "from newsrelations.query_db.relation_query import DBQueryHandlerCoocc\n",
    "from newsrelations.helper_classes.synonym_handler import SynonymHandler\n",
    "from newsrelations.metrics.distances import DistanceMeasure\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import models, corpora, similarities\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"stopwords\")\n",
    "tokenizer = nltk.data.load(\n",
    "    \"/home/jonas/anaconda3/lib/python3.7/nltk_data/punkt/english.pickle\"\n",
    ")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# extend stopwords with typical stopwords from news-paper articles\n",
    "extend_stopwords = [\n",
    "    \"also\",\n",
    "    \"said\",\n",
    "    \"one\",\n",
    "    \"new\",\n",
    "    \"two\",\n",
    "    \"says\",\n",
    "    \"could\",\n",
    "    \"would\",\n",
    "    \"should\",\n",
    "    \"three\",\n",
    "    \"three\",\n",
    "    \"like\",\n",
    "]\n",
    "for word in extend_stopwords:\n",
    "    stop_words.add(word)\n",
    "\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_contingency_table_from_single_topic(\n",
    "    relation_models_path, relation_models, topic_of_interest, no_entities=10\n",
    "):\n",
    "    \"\"\"\n",
    "    This function builds a contingency table from a input list of relation models generated with relation_miner.py \n",
    "    in regards to predetermined topic [TOPIC_OF_INTEREST].\n",
    "    The first model in [RELATION_MODELS] is the reference model all other models will be compared with.\n",
    "    The function extracts the top [NO_ENTITIES] co_occuring entities from the model and builds a contingency table.\n",
    "    \n",
    "    input:  relation_models = list\n",
    "            relation_models_path = str \n",
    "            topic_of_interest = str           \n",
    "            no_entities = int (standard 10)\n",
    "            \n",
    "    output: contingency_table = pandas DataFrame [rows = different models, columns = entities]\n",
    "    \"\"\"\n",
    "    E1_SYNSET = 0\n",
    "    E2_SYNSET = 1\n",
    "\n",
    "    # initialize DistanceMeasure with reference-model\n",
    "    dm = DistanceMeasure(relation_models_path, str(relation_models[0]))\n",
    "\n",
    "    # extract top NO_ENTITIES entities\n",
    "    top = dm.get_top_co_occurrences(\n",
    "        topic_of_interest,\n",
    "        cutoff=no_entities,\n",
    "        e1_is_synset=E1_SYNSET,\n",
    "        e2_is_synset=E2_SYNSET,\n",
    "    )\n",
    "    # write first row of contingency_table\n",
    "    contingency_table = pd.DataFrame(\n",
    "        np.array([t[1] for t in top]),\n",
    "        index=[t[0] for t in top],\n",
    "        columns=[str(relation_models[0])],\n",
    "    )\n",
    "\n",
    "    # loop through all remaining models\n",
    "    for model in relation_models[1:]:\n",
    "        # initialize db_handler()\n",
    "        db_handler = DBQueryHandlerCoocc(relation_models_path, model)\n",
    "\n",
    "        # buffer for cooccurrences\n",
    "        co_occs = []\n",
    "        # loop through all entities and get number of co-occurrences\n",
    "        for row in contingency_table.index:\n",
    "            co_occs.append(\n",
    "                len(\n",
    "                    list(\n",
    "                        db_handler.select_relations(\n",
    "                            e1=topic_of_interest.lower(),\n",
    "                            e2=row.lower(),\n",
    "                            e1_is_synset=E1_SYNSET,\n",
    "                            e2_is_synset=E2_SYNSET,\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        contingency_table[str(model)] = co_occs\n",
    "\n",
    "    # transpose the contingency table to get it into the right format\n",
    "    contingency_table = contingency_table.transpose()\n",
    "    return contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_contingency_table_from_preset_cooccurrences(\n",
    "    relation_models_path, model, cooccurrences_list\n",
    "):\n",
    "    \"\"\"\n",
    "    This function builds a contingency table from a input list of relation models generated with relation_miner.py \n",
    "    in regards to predetermined co_occurrences [co_occurrences_list].\n",
    "    The first model in [RELATION_MODELS] is the reference model all other models will be compared with.\n",
    "    The function extracts the top [NO_ENTITIES] co_occuring entities from the model and builds a contingency table.\n",
    "    \n",
    "    input:  relation_models = list\n",
    "            relation_models_path = str \n",
    "            co_occurrences_list = list of tuples\n",
    "            no_entities = int (standard 10)\n",
    "            \n",
    "    output: contingency_table = pandas DataFrame [rows = different models, columns = entities]\n",
    "    \"\"\"\n",
    "    E1_SYNSET = 0\n",
    "    E2_SYNSET = 0\n",
    "\n",
    "    # write first row of contingency_table\n",
    "    contingency_table = pd.DataFrame(columns=[str(model)])\n",
    "\n",
    "    # initialize db_handler()\n",
    "    db_handler = DBQueryHandlerCoocc(relation_models_path, model)\n",
    "\n",
    "    # buffer for cooccurrences\n",
    "    co_occs = []\n",
    "    # loop through all entities and get number of co-occurrences\n",
    "    for single_tuple in cooccurrences_list:\n",
    "        co_occs.append(\n",
    "            len(\n",
    "                list(\n",
    "                    db_handler.select_relations(\n",
    "                        e1=single_tuple[0].lower(),\n",
    "                        e2=single_tuple[1].lower(),\n",
    "                        e1_is_synset=E1_SYNSET,\n",
    "                        e2_is_synset=E2_SYNSET,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    contingency_table[str(model)] = co_occs\n",
    "\n",
    "    # transpose the contingency table to get it into the right format\n",
    "    contingency_table = contingency_table.transpose()\n",
    "    return contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_models(relation_models):\n",
    "    \"\"\"\n",
    "    This function lists the models from the relation_models list and their counter index\n",
    "    \n",
    "    Input: relation_models = list\n",
    "    \n",
    "    Output: None\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    for model in relation_models:\n",
    "        print(\"model(\" + str(i) + \"): \" + str(model))\n",
    "        i += 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_contingency_table_from_topic_list(\n",
    "    relation_models_path, relation_models, topic_of_interest_list, no_entities=10\n",
    "):\n",
    "    \"\"\"\n",
    "    This function builds a contingency table from a input list of relation models generated with relation_miner.py \n",
    "    in regards to predetermined topic list [topic_of_interest_list].\n",
    "    The first model in [relation_models] is the reference model all other models will be compared with.\n",
    "    The function extracts the top [no_entities] co-occuring entities for the first [topic_of_interest] from the \n",
    "    model and builds a contingency table.\n",
    "    \n",
    "    \n",
    "    input:  relation_models = list\n",
    "            relation_models_path = str \n",
    "            topic_of_interest_list = list           \n",
    "            no_entities = int (standard 10)\n",
    "            \n",
    "    output: contingency_table = pandas DataFrame [rows = different models, columns = entities]\n",
    "    \"\"\"\n",
    "    # identifier for models in relation_models_list\n",
    "    i = 0\n",
    "    # print models with idx\n",
    "    explain_models(relation_models)\n",
    "\n",
    "    # initialize DistanceMeasure with reference-model\n",
    "    dm = DistanceMeasure(relation_models_path, str(relation_models[0]))\n",
    "\n",
    "    # extract top NO_ENTITIES entities\n",
    "    top = dm.get_top_co_occurrences(\n",
    "        topic_of_interest_list[0], cutoff=no_entities, e1_is_synset=0, e2_is_synset=0\n",
    "    )\n",
    "    # write first row of contingency_table\n",
    "    contingency_table = pd.DataFrame(\n",
    "        np.array([t[1] for t in top]),\n",
    "        index=[t[0] for t in top],\n",
    "        columns=[str(topic_of_interest_list[0]) + \" (\" + str(i) + \")\"],\n",
    "    )\n",
    "\n",
    "    # loop through the models\n",
    "    for model in relation_models[:]:\n",
    "        # initialize db_handler()\n",
    "        db_handler = DBQueryHandlerCoocc(relation_models_path, model)\n",
    "\n",
    "        for topic in topic_of_interest_list:\n",
    "            # buffer for co-occurrencces\n",
    "            co_occs = []\n",
    "\n",
    "            # loop through all all entities and get number of co-occurrences\n",
    "            for row in contingency_table.index:\n",
    "                co_occs.append(\n",
    "                    len(\n",
    "                        list(\n",
    "                            db_handler.select_relations(\n",
    "                                e1=topic.lower(),\n",
    "                                e2=row.lower(),\n",
    "                                e1_is_synset=0,\n",
    "                                e2_is_synset=0,\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            contingency_table[str(topic) + \" (\" + str(i) + \")\"] = co_occs\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # transpose the contingency table to get it into the right format\n",
    "    contingency_table = contingency_table.transpose()\n",
    "\n",
    "    return contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_entity_lists(\n",
    "    relation_models_path, relation_models, topic_list1, topic_list2\n",
    "):\n",
    "    \"\"\"\n",
    "    This function creates a contingency table from a two input lists of entities [topic_list1][topic_list2]\n",
    "    from a list of relation models \n",
    "    \n",
    "    input:  relation_models = list\n",
    "            relation_models_path = str \n",
    "            topic_list1 = list           \n",
    "            topic_list2 = list\n",
    "            \n",
    "    output: contingency_table = pandas DataFrame [rows = different models, columns = entities]\n",
    "    \"\"\"\n",
    "    # idx for models\n",
    "    i = 0\n",
    "\n",
    "    # print models with idx\n",
    "    explain_models(relation_models)\n",
    "\n",
    "    # initialize contingency_table\n",
    "    contingency_table = pd.DataFrame(index=topic_list2)\n",
    "\n",
    "    # loop through the models\n",
    "    for model in relation_models:\n",
    "        # initialize db_handler()\n",
    "        db_handler = DBQueryHandlerCoocc(relation_models_path, model)\n",
    "\n",
    "        for topic1 in topic_list1:\n",
    "            # buffer for co-occurrencces\n",
    "            co_occs = []\n",
    "\n",
    "            # loop through all all entities and get number of co-occurrences\n",
    "            for topic2 in topic_list2:\n",
    "                co_occs.append(\n",
    "                    len(\n",
    "                        list(\n",
    "                            db_handler.select_relations(\n",
    "                                e1=topic1.lower(),\n",
    "                                e2=topic2.lower(),\n",
    "                                e1_is_synset=0,\n",
    "                                e2_is_synset=0,\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            contingency_table[str(topic1) + \" (\" + str(i) + \")\"] = co_occs\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # transpose the contingency table to get it into the right format\n",
    "    contingency_table = contingency_table.transpose()\n",
    "\n",
    "    return contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_idf_table(relation_models_path, model):\n",
    "    \"\"\"\n",
    "    This function calculates the TF-IDF value over all words mentioned in the [model]s corpus.\n",
    "    \n",
    "    input:      relation_models_path = string\n",
    "                realtion_model = string\n",
    "    \n",
    "    output:     df = DataFrame (containing all tf-idf-values for each word)\n",
    "    \"\"\"\n",
    "    # number of top words in corpus\n",
    "    N = 10\n",
    "    \n",
    "    # initialize db_handler\n",
    "    db_handler = DBQueryHandlerCoocc(relation_models_path, model)\n",
    "    # retrieve all articles from database into article\n",
    "    articles = db_handler.get_articles()\n",
    "\n",
    "\n",
    "    preprocessed_articles = \"\"\n",
    "\n",
    "    # preprocessing of articles\n",
    "    for article in tqdm(articles):\n",
    "        # get text, lower it and tokenize it into sentences, strip punctuation and stop words\n",
    "        single_article = str(article.text)\n",
    "        single_article = single_article.lower()\n",
    "        single_article = word_tokenize(single_article)\n",
    "        single_article = [word for word in single_article if word.isalnum()]\n",
    "        single_article = [word for word in single_article if not word in stop_words]\n",
    "\n",
    "        # concat everything into a string again \n",
    "        art = \"\"\n",
    "        for word in single_article:\n",
    "            art = art + \" \" + word\n",
    "        # append to corpus-string   \n",
    "        preprocessed_articles = preprocessed_articles + \" \" + art    \n",
    "    preprocessed_articles = [preprocessed_articles]\n",
    "    \n",
    "    # calculate TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(preprocessed_articles)\n",
    "    names = vectorizer.get_feature_names()\n",
    "    data = vectors.todense().tolist()\n",
    "\n",
    "    # Create a dataframe with the results\n",
    "    df = pd.DataFrame(data, columns=names)\n",
    "    df = df[filter(lambda x: x not in list(stop_words) , df.columns)]\n",
    "    \"\"\"\n",
    "    df\n",
    "\n",
    "    for i in df.iterrows():\n",
    "        print(i[1].sort_values(ascending=False)[:N])\n",
    "    \"\"\"    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared(contingency_table, print_orig = False, print_expect = False, print_chi_contr = False):\n",
    "    \"\"\"\n",
    "    This function conducts a chi-squared test of independence between the different rows of a contingency table\n",
    "    \n",
    "    input: contingency_table\n",
    "    \n",
    "    ouput: None\n",
    "    \"\"\"\n",
    "    contingency_table = sm.stats.Table(contingency_table)\n",
    "    results = contingency_table.test_nominal_association()\n",
    "    \n",
    "    \n",
    "    # orig contingency table\n",
    "    if print_orig == True:\n",
    "        print(\"Original contingency table:\")\n",
    "        print(contingency_table.table_orig)\n",
    "    # expected values\n",
    "    if print_expect == True:\n",
    "        print(\"\\nExpected values:\")\n",
    "        print(contingency_table.fittedvalues)\n",
    "    # chi-squared contributions\n",
    "    if print_chi_contr == True:\n",
    "        print(\"\\nChi-square contributions:\")\n",
    "        print(contingency_table.chi2_contribs)\n",
    "    \n",
    "    # results\n",
    "    print(\"\\nResults:\")\n",
    "    print(results)\n",
    "\n",
    "   \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_chi_squared_comparison(\n",
    "    relation_models_path, relation_models, topic_of_interest, no_entities\n",
    "):\n",
    "    \"\"\"\n",
    "    This function extracts chi-squared test results for all combinations of news-outlets\n",
    "    \n",
    "    input: contingency_table\n",
    "    \n",
    "    ouput: None\n",
    "    \"\"\"\n",
    "\n",
    "    # extracting a contingency table from a single reference entity\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    df_results = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(relation_models)):\n",
    "        for j in range(len(relation_models)):\n",
    "            models = []\n",
    "            models = [relation_models[i]] + [relation_models[j]]\n",
    "\n",
    "            contingency_table = build_contingency_table_from_single_topic(\n",
    "                relation_models_path, models, topic_of_interest, no_entities\n",
    "            )\n",
    "\n",
    "            contingency_table = sm.stats.Table(contingency_table)\n",
    "            results = contingency_table.test_nominal_association()\n",
    "\n",
    "            df_results[\n",
    "                str(relation_models[i][-10:-7])\n",
    "                + \" - \"\n",
    "                + str(relation_models[j][-10:-7])\n",
    "            ] = [\n",
    "                results.statistic,\n",
    "                results.pvalue,\n",
    "            ]\n",
    "\n",
    "    df_results = df_results.transpose()\n",
    "    df_results = df_results.rename(columns={0: \"chi_sq\", 1: \"p_value\"})\n",
    "    df_results\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_sentiment(reference_model):\n",
    "    \"\"\"\n",
    "    This function calculates the sentiment of the top co-occurring entities between a reference news outlet \n",
    "    [reference_model] and other models. The sentiment is calculated of sentences which contain the co-occurring entity\n",
    "    and neighbouring sentences within the scope of [SCOPE] sentences.\n",
    "    Returns a dataframe with all necessary information\n",
    "    \n",
    "    Input:    reference_model (string)\n",
    "              models (list)\n",
    "              topic_of_interest (string)\n",
    "    \n",
    "    Output:   df_results (df)\n",
    "    \"\"\"\n",
    "    models = RELATION_MODELS\n",
    "    topic_of_interest = TOPIC_OF_INTEREST\n",
    "\n",
    "    # initialize sentiment ananlyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    # extract list of top co-occurring entities\n",
    "    dm = DistanceMeasure(RELATION_MODELS_PATH, str(reference_model))\n",
    "    top = dm.get_top_co_occurrences(\n",
    "        topic_of_interest, cutoff=NO_ENTITIES, e1_is_synset=0, e2_is_synset=1,\n",
    "    )\n",
    "    for i in range(len(top)):\n",
    "        top[i] = top[i][0][:-5]\n",
    "        top[i] = top[i].replace(\"_\", \" \")\n",
    "\n",
    "    df_results = pd.DataFrame()\n",
    "\n",
    "    # loop through all models\n",
    "    for model in tqdm(models):\n",
    "        if model == reference_model:\n",
    "            continue\n",
    "        else:\n",
    "            # initialize db_handler\n",
    "            description = str(str(reference_model[-10:-7]) + \"-\" + str(model[-10:-7]))\n",
    "            db_handler = DBQueryHandlerCoocc(RELATION_MODELS_PATH, model)\n",
    "            for topic in top:\n",
    "                # retrieve co-occurrences without synsets\n",
    "                articles = db_handler.get_articles_by_substring_text(topic)\n",
    "                # convert generator (for count)\n",
    "                articles = list(articles)\n",
    "\n",
    "                sentiment_of_topic = []\n",
    "                try:\n",
    "                    for single_article in articles:\n",
    "                        # get text, lower it and tokenize it into sentences\n",
    "                        article = single_article.text\n",
    "                        article = article.lower()\n",
    "                        article_tokenized = tokenizer.tokenize(article)\n",
    "\n",
    "                        # append buffers in the beginning and the end, in case the scope is overflowing\n",
    "                        for i in range(0, SCOPE):\n",
    "                            article_tokenized.append(\"buffer\")\n",
    "                            article_tokenized.insert(0, \"buffer\")\n",
    "\n",
    "                        # extract compound sentiment score of each sector in a scope around the sentence\n",
    "                        # in which the entity is mentioned\n",
    "                        for line in range(0, len(article_tokenized)):\n",
    "                            if TOPIC_OF_INTEREST in article_tokenized[line]:\n",
    "                                sector = []\n",
    "                                for i in range(line - SCOPE, line + 1 + SCOPE):\n",
    "                                    sentiment = sid.polarity_scores(\n",
    "                                        article_tokenized[i]\n",
    "                                    )\n",
    "                                    sector.append(sentiment[\"compound\"])\n",
    "\n",
    "                                # final sentiment as mean for the sector\n",
    "                                final_sentiment = statistics.mean(sector)\n",
    "                                # sentiment of topic as list for all final sentiments\n",
    "                                sentiment_of_topic.append(final_sentiment)\n",
    "\n",
    "                    # extract parameters and print\n",
    "                    mean_sentiment = statistics.mean(sentiment_of_topic)\n",
    "                    median_sentiment = statistics.median(sentiment_of_topic)\n",
    "                    \"\"\"\n",
    "                    print(\"=\" * 100)\n",
    "                    print(\"outlet: \" + description)\n",
    "                    print(\"topic: \" + topic)\n",
    "                    print(\"mean sentiment: \" + str(mean_sentiment))\n",
    "                    print(\"median sentiment: \" + str(median_sentiment))\n",
    "                    print(\"\\n\")\n",
    "                    \"\"\"\n",
    "                    results = [\n",
    "                        [\n",
    "                            str(reference_model[-10:-7]),\n",
    "                            str(model[-10:-7]),\n",
    "                            topic,\n",
    "                            mean_sentiment,\n",
    "                            median_sentiment,\n",
    "                        ]\n",
    "                    ]\n",
    "                    df_results = df_results.append(results)\n",
    "                except:\n",
    "                    fail = [\n",
    "                        [\n",
    "                            str(reference_model[-10:-7]),\n",
    "                            str(model[-10:-7]),\n",
    "                            topic,\n",
    "                            \"NaN\",\n",
    "                            \"NaN\",\n",
    "                        ]\n",
    "                    ]\n",
    "                    df_results = df_results.append(fail)\n",
    "    df_results = df_results.rename(\n",
    "        columns={\n",
    "            0: \"reference\",\n",
    "            1: \"comparison\",\n",
    "            2: \"topic\",\n",
    "            3: \"mean_sentiment\",\n",
    "            4: \"median_sentiment\",\n",
    "        }\n",
    "    )\n",
    "    df_results.reset_index()\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kmeans_on_sentiment(df):\n",
    "    \"\"\"\n",
    "    This function runs a k-mean clustering on the sentiment vectors of each given input news-outlet from\n",
    "    the input df and returns a dataframe with the labels of the calculated clusters per entity.\n",
    "\n",
    "    input: df(Dataframe)\n",
    "    \n",
    "    output: df_res(Dataframe)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # splitting the df into list of dfs containing only one reference\n",
    "    df_list = [df.loc[i : i + 80 - 1, :] for i in range(0, len(df), 80)]\n",
    "\n",
    "    # list of dataframes, that contain the sorted mean_sentiments of comparisons per reference-news-outlet\n",
    "    entity_frame_list = []\n",
    "\n",
    "    for df_sublist in df_list:\n",
    "        df_sublist = df_sublist.drop(columns=[\"reference\", \"median_sentiment\"])\n",
    "        df_sublist = df_sublist.reset_index(drop=True)\n",
    "\n",
    "        df_subsublist = [\n",
    "            df_sublist.loc[j : j + 10 - 1, :] for j in range(0, len(df_sublist), 10)\n",
    "        ]\n",
    "\n",
    "        entity = pd.DataFrame()\n",
    "\n",
    "        for k in range(len(df_subsublist)):\n",
    "            df_subsublist[k] = df_subsublist[k].reset_index(drop=True)\n",
    "            s = df_subsublist[k][\"comparison\"][0]\n",
    "            df_subsublist[k].index = df_subsublist[k][\"topic\"]\n",
    "            df_subsublist[k] = df_subsublist[k].transpose()\n",
    "            df_subsublist[k] = df_subsublist[k].drop(\"topic\")\n",
    "            df_subsublist[k] = df_subsublist[k].drop(\"comparison\")\n",
    "            df_subsublist[k].index = [s]\n",
    "            entity = entity.append(df_subsublist[k])\n",
    "\n",
    "        entity_frame_list.append(entity)\n",
    "\n",
    "    # run k-means on the the sentiment vectors for each news-outlet and cluster into three groups\n",
    "    df_res = pd.DataFrame(\n",
    "        index=[\"NYT\", \"HFP\", \"WPO\", \"CNN\", \"RET\", \"NBC\", \"CTB\", \"FXN\", \"WSJ\"]\n",
    "    )\n",
    "\n",
    "    for df in entity_frame_list:\n",
    "        # NaNs are taken as neutral sentiment 0\n",
    "        df = df.fillna(0)\n",
    "        # clustering\n",
    "        kmeans = KMeans(n_clusters=3).fit(df)\n",
    "        # centroids = kmeans.cluster_centers_\n",
    "        df[\"labels\"] = kmeans.labels_\n",
    "\n",
    "        df_res = pd.concat([df_res, df.labels], axis=1, sort=False)\n",
    "\n",
    "    return df_res, entity_frame_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib, ssl\n",
    "from getpass import getpass\n",
    "\n",
    "smtp_server = \"smtp.gmail.com\"\n",
    "port = 587  # For starttls\n",
    "sender_email = \"pythonserver.jonas.m.ehrhardt@gmail.com\"\n",
    "password = getpass()\n",
    "\n",
    "# Create a secure SSL context\n",
    "context = ssl.create_default_context()\n",
    "\n",
    "\n",
    "def message_me(calculation_ID):\n",
    "    \"\"\"\n",
    "    This function sends me an email, when called, with the calculation ID given in input ;) \n",
    "    \"\"\"\n",
    "    # Try to log in to server and send email\n",
    "    try:\n",
    "        server = smtplib.SMTP(smtp_server, port)\n",
    "        server.ehlo()  # Can be omitted\n",
    "        server.starttls(context=context)  # Secure the connection\n",
    "        server.ehlo()  # Can be omitted\n",
    "        server.login(sender_email, password)\n",
    "\n",
    "        sender = \"pythonserver.jonas.m.ehrhardt@gmail.com\"\n",
    "        receivers = [\"jonas@xorentec.de\"]\n",
    "\n",
    "        message = (\n",
    "            \"From: Jonas Pythonserver\\nSubject: Calculation finished\\nThe Calculation \"\n",
    "            + calculation_ID\n",
    "            + \" is finished...\"\n",
    "        )\n",
    "\n",
    "        server.sendmail(sender, receivers, message)\n",
    "\n",
    "        print(\"Successfully sent email\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        server.quit()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_me(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Final datasets <a class=\"anchor\" id=\"finalDatasets\"></a>\n",
    "\n",
    "Datasets used for the thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for parameter tuning\n",
    "# for the estiation of parameters number of entities and topic_of_interest\n",
    "#\n",
    "# -year:    2012\n",
    "# -domain:  news\n",
    "# -sources(left):    Huffington Post (HFP) 4909, New York Times (NYT) 2541,\n",
    "#         (center):  CNN (CNN) 2491, Reuters (RET) 2135\n",
    "#         (right):   FoxNews (FXN) 3784, (WSJ) 1215\n",
    "\n",
    "# directory path of relation models\n",
    "RELATION_MODELS_PATH = (\n",
    "    \"/home/jonas/Documents/GitHub/MasterThesis/models/calibrationDataset\"\n",
    ")\n",
    "\n",
    "# model names\n",
    "RELATION_MODELS = [\n",
    "    \"RM_2012_news_HFP.sqlite\",\n",
    "    \"RM_2012_news_NYT.sqlite\",\n",
    "    \"RM_2012_news_CNN.sqlite\",\n",
    "    \"RM_2012_news_RET.sqlite\",\n",
    "    \"RM_2012_news_FXN.sqlite\",\n",
    "    \"RM_2012_news_WSJ.sqlite\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Dataset\n",
    "# final daset based on works of Budak (2016) Flaxmann (2016) and Groseclose (2015)\n",
    "#\n",
    "# -year:    2011\n",
    "# -domain:  news\n",
    "# -sources(left):    Huffington Post (HFP) 14876, LA Times (LAT) 445, New York Times (NYT) 11281,\n",
    "#                    Washington Post (WP) 14814, Daily KOS (DKO) 123\n",
    "#         (center):  BBC (BBC) 52, CNN (CNN) 2652, Reuters (RET) 16767, Yahoo News (YHN) 211\n",
    "#         (right):   Chicago Tribune (CTB) 2843, FoxNews (FXN) 6508, NBC (NBC) 3958, USA Today (UST) 171\n",
    "#                    Wall Street Journal (WSJ) 2522, Breitbart (BBT) 76\n",
    "\n",
    "# directory path of relation models\n",
    "RELATION_MODELS_PATH = \"/home/jonas/Documents/GitHub/MasterThesis/models/finalDataset\"\n",
    "\n",
    "# model names\n",
    "RELATION_MODELS = [\n",
    "    \"RM_2011_news_BBC.sqlite\",\n",
    "    \"RM_2011_news_BBT.sqlite\",\n",
    "    \"RM_2011_news_CNN.sqlite\",\n",
    "    \"RM_2011_news_CTB.sqlite\",\n",
    "    \"RM_2011_news_DKO.sqlite\",\n",
    "    \"RM_2011_news_FXN.sqlite\",\n",
    "    \"RM_2011_news_HFP.sqlite\",\n",
    "    \"RM_2011_news_LAT.sqlite\",\n",
    "    \"RM_2011_news_NBC.sqlite\",\n",
    "    \"RM_2011_news_NYT.sqlite\",\n",
    "    \"RM_2011_news_RET.sqlite\",\n",
    "    \"RM_2011_news_UST.sqlite\",\n",
    "    \"RM_2011_news_WPO.sqlite\",\n",
    "    \"RM_2011_news_WSJ.sqlite\",\n",
    "    \"RM_2011_news_YHN.sqlite\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Parameter tuning <a class=\"anchor\" id=\"parameterTuning\"></a>\n",
    "\n",
    "All parameter tuning is run with the [calibration dataset](#finalDatasets).\n",
    "The categorization in slant groups is derived from literature.\n",
    "\n",
    "\n",
    "## 4.1 n-entity estimation <a class=\"anchor\" id=\"nEntity\"></a>\n",
    "\n",
    "Here I estimated the number of co-occurring entities (n) with the calibration dataset. Therefore I observe the curve of the p-value of the chi-squared test over an increasing number of n. \n",
    "For same-slant group news-outlets the p-value is expected to be very low, for different-slant news-outlets it is expected to be high (check [hypothesis 1](#hyp1) for explanation). \n",
    "For a high n the p-value of the different-slant news-outlets is expected to decrease again. Hence the optimal n  has a high count for p-values below 0.05 for same-slant outlets and a high count for p-values above 0.05 for different-slant news-outlets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset and set hyper-parameters\n",
    "\n",
    "# directory path of relation models\n",
    "RELATION_MODELS_PATH = (\n",
    "    \"/home/jonas/Documents/GitHub/MasterThesis/models/calibrationDataset\"\n",
    ")\n",
    "\n",
    "# model names\n",
    "RELATION_MODELS = [\n",
    "    \"RM_2012_news_HFP.sqlite\",\n",
    "    \"RM_2012_news_NYT.sqlite\",\n",
    "    \"RM_2012_news_CNN.sqlite\",\n",
    "    \"RM_2012_news_RET.sqlite\",\n",
    "    \"RM_2012_news_FXN.sqlite\",\n",
    "    \"RM_2012_news_WSJ.sqlite\",\n",
    "]\n",
    "\n",
    "# path to results folder\n",
    "RESULTS_PATH = \"/home/jonas/Documents/GitHub/MasterThesis/results/\"\n",
    "\n",
    "# max number of entities + 1, to compare with the reference entity\n",
    "NO_ENTITIES = 31\n",
    "\n",
    "# reference entity -\n",
    "TOPIC_OF_INTEREST = \"obama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate optimal n - within same-slant-constellations\n",
    "models = [\n",
    "    [RELATION_MODELS[0], RELATION_MODELS[1]],\n",
    "    [RELATION_MODELS[1], RELATION_MODELS[0]],\n",
    "    [RELATION_MODELS[2], RELATION_MODELS[3]],\n",
    "    [RELATION_MODELS[3], RELATION_MODELS[2]],\n",
    "    [RELATION_MODELS[4], RELATION_MODELS[5]],\n",
    "    [RELATION_MODELS[5], RELATION_MODELS[4]],\n",
    "]\n",
    "\n",
    "# initialize dataframes\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# loop through model constellation in models list\n",
    "def estimate_n_same_slant(models):\n",
    "    df_results = pd.DataFrame()\n",
    "    description = str(str(models[0][-10:-7]) + \"-\" + str(models[1][-10:-7]))\n",
    "    # loop through entity numbers until max entity is reached\n",
    "    for n in tnrange(1, NO_ENTITIES, desc=description):\n",
    "        # create SQL query and build contingency table for sm.stats\n",
    "        contingency_table = build_contingency_table_from_single_topic(\n",
    "            RELATION_MODELS_PATH, models, TOPIC_OF_INTEREST, n\n",
    "        )\n",
    "        contingency_table = sm.stats.Table(contingency_table)\n",
    "        # calculate results + add them to dataframe\n",
    "        results = contingency_table.test_nominal_association()\n",
    "        df_results[n] = [results.pvalue]\n",
    "\n",
    "    df_results = df_results.transpose()\n",
    "    df_results = df_results.rename(columns={0: description})\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "\n",
    "# Init multiprocessing.Pool()\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "# concat the pool-parallelized dataframes from pathlist\n",
    "df = pd.concat(pool.map(estimate_n_same_slant, [model for model in models]), axis=1)\n",
    "\n",
    "# close loop\n",
    "pool.close()\n",
    "\n",
    "\n",
    "# save results to csv\n",
    "df.to_csv(\n",
    "    RESULTS_PATH + \"nSameSlant_\" + TOPIC_OF_INTEREST + \".csv\", index=False\n",
    ")\n",
    "\n",
    "message_me(\"nSameSlant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "df_same = pd.read_csv(RESULTS_PATH + \"nSameSlant_\" + TOPIC_OF_INTEREST + \".csv\")\n",
    "\n",
    "# Visualze dataset\n",
    "df_same.plot(kind=\"line\", figsize=(7.5, 7))\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.title(\n",
    "    \"Behavior of p-values in relation to number of examined entities\\n within same slant groups - reference entitiy: \"\n",
    "    + TOPIC_OF_INTEREST\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate optimal n - within different-slant-constellations\n",
    "models = [\n",
    "    [RELATION_MODELS[0], RELATION_MODELS[2]],\n",
    "    [RELATION_MODELS[0], RELATION_MODELS[3]],\n",
    "    [RELATION_MODELS[0], RELATION_MODELS[4]],\n",
    "    [RELATION_MODELS[0], RELATION_MODELS[5]],\n",
    "    [RELATION_MODELS[1], RELATION_MODELS[2]],\n",
    "    [RELATION_MODELS[1], RELATION_MODELS[3]],\n",
    "    [RELATION_MODELS[1], RELATION_MODELS[4]],\n",
    "    [RELATION_MODELS[1], RELATION_MODELS[5]],\n",
    "    [RELATION_MODELS[2], RELATION_MODELS[0]],\n",
    "    [RELATION_MODELS[2], RELATION_MODELS[1]],\n",
    "    [RELATION_MODELS[2], RELATION_MODELS[4]],\n",
    "    [RELATION_MODELS[2], RELATION_MODELS[5]],\n",
    "    [RELATION_MODELS[3], RELATION_MODELS[0]],\n",
    "    [RELATION_MODELS[3], RELATION_MODELS[1]],\n",
    "    [RELATION_MODELS[3], RELATION_MODELS[4]],\n",
    "    [RELATION_MODELS[3], RELATION_MODELS[5]],\n",
    "    [RELATION_MODELS[4], RELATION_MODELS[0]],\n",
    "    [RELATION_MODELS[4], RELATION_MODELS[1]],\n",
    "    [RELATION_MODELS[4], RELATION_MODELS[2]],\n",
    "    [RELATION_MODELS[4], RELATION_MODELS[3]],\n",
    "    [RELATION_MODELS[5], RELATION_MODELS[0]],\n",
    "    [RELATION_MODELS[5], RELATION_MODELS[1]],\n",
    "    [RELATION_MODELS[5], RELATION_MODELS[2]],\n",
    "    [RELATION_MODELS[5], RELATION_MODELS[3]],\n",
    "]\n",
    "\n",
    "# initialize dataframes and counter for column name\n",
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "def estimate_n_diff_slant(models):\n",
    "    # loop through model constellation in models list\n",
    "    # for constellation in tqdm(models):\n",
    "    df_results = pd.DataFrame()\n",
    "    description = str(str(models[0][-10:-7]) + \"-\" + str(models[1][-10:-7]))\n",
    "    # loop through entity numbers until max entity is reached\n",
    "    for n in tnrange(1, NO_ENTITIES, desc=description):\n",
    "\n",
    "        # create SQL query and build contingency table for sm.stats\n",
    "        contingency_table = build_contingency_table_from_single_topic(\n",
    "            RELATION_MODELS_PATH, models, TOPIC_OF_INTEREST, n\n",
    "        )\n",
    "        contingency_table = sm.stats.Table(contingency_table)\n",
    "        # calculate results + add them to dataframe\n",
    "        results = contingency_table.test_nominal_association()\n",
    "        df_results[n] = [results.pvalue]\n",
    "\n",
    "    df_results = df_results.transpose()\n",
    "    df_results = df_results.rename(columns={0: description})\n",
    "\n",
    "    return df_results\n",
    "\n",
    "\n",
    "# Init multiprocessing.Pool()\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "# concat the pool-parallelized dataframes from pathlist\n",
    "df = pd.concat(pool.map(estimate_n_diff_slant, [model for model in models]), axis=1)\n",
    "\n",
    "# close loop\n",
    "pool.close()\n",
    "\n",
    "\n",
    "# save results to csv\n",
    "df.to_csv(\n",
    "    RESULTS_PATH + \"nDiffSlant_\" + TOPIC_OF_INTEREST + \".csv\", index=False\n",
    ")\n",
    "\n",
    "message_me(\"nDiffSlant_All\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "df_diff = pd.read_csv(RESULTS_PATH + \"nDiffSlant_\" + TOPIC_OF_INTEREST + \".csv\")\n",
    "\n",
    "# Visualze\n",
    "df_diff.plot(kind=\"line\", figsize=(7.5, 7.0))\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.title(\n",
    "    \"Behavior of p-values in relation to number of examined entities \\nbetween different slant groups - reference entity: \"\n",
    "    + TOPIC_OF_INTEREST\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Reference entity estimation <a class=\"anchor\" id=\"refEntity\"></a>\n",
    "\n",
    "To estimate the optimal reference entity for the co-occurrences, the behavior of different and same-slant news-outlets are compared. A good reference entity gets high p-values for different-slant news-outlets and low p-values for same-slant news-outlets.\n",
    "The tested entities are derived from Gentzkow and Shapiro (2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "# directory path of relation models\n",
    "RELATION_MODELS_PATH = (\n",
    "    \"/home/jonas/Documents/GitHub/MasterThesis/models/calibrationDataset\"\n",
    ")\n",
    "\n",
    "# model names\n",
    "RELATION_MODELS = [\n",
    "    \"RM_2012_news_HFP.sqlite\",\n",
    "    \"RM_2012_news_NYT.sqlite\",\n",
    "    \"RM_2012_news_CNN.sqlite\",\n",
    "    \"RM_2012_news_RET.sqlite\",\n",
    "    \"RM_2012_news_FXN.sqlite\",\n",
    "    \"RM_2012_news_WSJ.sqlite\",\n",
    "]\n",
    "\n",
    "# path to results folder\n",
    "RESULTS_PATH = \"/home/jonas/Documents/GitHub/MasterThesis/results/\"\n",
    "\n",
    "# number of entities to compare with the reference entitiy\n",
    "NO_ENTITIES = 16\n",
    "\n",
    "# reference entities\n",
    "TOPIC_OF_INTEREST_LIST = [\n",
    "    \"sport\",\n",
    "    \"obama\",\n",
    "    \"bush\",\n",
    "    \"united_states\",\n",
    "    \"healthcare\",\n",
    "    \"insurance\",\n",
    "    \"gun\",\n",
    "    \"school\",\n",
    "    \"death_tax\",\n",
    "    \"terrorism\",\n",
    "    \"Germany\",\n",
    "    \"France\",\n",
    "    \"health_care\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataframes and counter for column name\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# same-slant news-outlets\n",
    "models = [RELATION_MODELS[0], RELATION_MODELS[1]]\n",
    "# different-slant news-outlets\n",
    "# models = [RELATION_MODELS[0], RELATION_MODELS[4]]\n",
    "\n",
    "def estimate_optimal_reference_entity(topic):\n",
    "    # loop through model constellation in models list\n",
    "    # for constellation in tqdm(models):\n",
    "    df_results = pd.DataFrame()\n",
    "    description = str(str(models[0][-10:-7]) + \"-\" + str(models[1][-10:-7]))\n",
    "    # loop through entity numbers until max entity is reached\n",
    "    for n in tnrange(1, NO_ENTITIES, desc=topic):\n",
    "        try:\n",
    "            # create SQL query and build contingency table for sm.stats\n",
    "            contingency_table = build_contingency_table_from_single_topic(\n",
    "                RELATION_MODELS_PATH, models, topic, n\n",
    "            )\n",
    "            contingency_table = sm.stats.Table(contingency_table)\n",
    "            # calculate results + add them to dataframe\n",
    "            results = contingency_table.test_nominal_association()\n",
    "            df_results[n] = [results.pvalue]\n",
    "        except:\n",
    "            print(\"There is too little data for \" + topic)\n",
    "            break\n",
    "\n",
    "    df_results = df_results.transpose()\n",
    "    df_results = df_results.rename(columns={0: topic})\n",
    "\n",
    "    return df_results\n",
    "\n",
    "\n",
    "# Init multiprocessing.Pool()\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "# concat the pool-parallelized dataframes from pathlist\n",
    "df = pd.concat(\n",
    "    pool.map(estimate_optimal_reference_entity, [topic for topic in TOPIC_OF_INTEREST_LIST]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# close pool\n",
    "pool.close()\n",
    "\n",
    "# save results to csv\n",
    "df.to_csv(RESULTS_PATH + \"reference_entity_estimation_diff.csv\", index=False)\n",
    "# df.to_csv(RESULTS_PATH + \"reference_entity_estimation_same.csv\", index=False)\n",
    "\n",
    "message_me(\"reference_estimation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and visualize - same-slant news-outlet\n",
    "df_same = pd.read_csv(RESULTS_PATH + \"reference_entity_estimation_same.csv\")\n",
    "\n",
    "df_same.plot(kind=\"line\", figsize=(7.5, 4))\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.title(\n",
    "    \"Behavior of p-values in relation to the chosen\\nreference entity - same-slant newsoutlet\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Import and visualize - diff-slant news-outlet\n",
    "df_diff = pd.read_csv(RESULTS_PATH + \"reference_entity_estimation_diff.csv\")\n",
    "\n",
    "df_diff.plot(kind=\"line\", figsize=(7.5, 4))\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.title(\n",
    "    \"Behavior of p-values in relation to the chosen\\nreference entity - different-slant newsoutlet\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5. Hypothesis testing <a class=\"anchor\" id=\"hypothesisTesting\"></a>\n",
    "\n",
    "## 5.1 Hypothesis 1<a class=\"anchor\" id=\"hyp1\"></a>\n",
    "\n",
    "1.1 Within same slant groups the co-occurring entities are independent from the news-outlet\n",
    "\n",
    "1.2 Between different slant groups the co-occurring entities are dependent from the news-outlet\n",
    "    \n",
    "To check hypothesis 1 I ran chi-squared tests for same (1.1) and different-slant (1.2) news-outlets.\n",
    "The results are matched with news-outlet groupings from literature. Accuracy, precision, recall and F1-score are estimated individually and combined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory path of relation models\n",
    "RELATION_MODELS_PATH = \"/home/jonas/Documents/GitHub/MasterThesis/models/finalDataset\"\n",
    "\n",
    "# model names\n",
    "RELATION_MODELS = [\n",
    "    \"RM_2011_news_CNN.sqlite\",\n",
    "    \"RM_2011_news_CTB.sqlite\",\n",
    "    \"RM_2011_news_FXN.sqlite\",\n",
    "    \"RM_2011_news_HFP.sqlite\",\n",
    "    \"RM_2011_news_NBC.sqlite\",\n",
    "    \"RM_2011_news_NYT.sqlite\",\n",
    "    \"RM_2011_news_RET.sqlite\",\n",
    "    \"RM_2011_news_WPO.sqlite\",\n",
    "    \"RM_2011_news_WSJ.sqlite\",\n",
    "]\n",
    "\n",
    "# path to results folder\n",
    "RESULTS_PATH = \"/home/jonas/Documents/GitHub/MasterThesis/results/\"\n",
    "\n",
    "# reference entity\n",
    "TOPIC_OF_INTEREST = \"SPECIFIC_ENTITY_last_last_approach_Z\"\n",
    "\n",
    "# number of co-occurring entities + 1\n",
    "NO_ENTITIES = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idfs = []\n",
    "\n",
    "for model in RELATION_MODELS:\n",
    "    df = create_tf_idf_table(RELATION_MODELS_PATH, model)\n",
    "    tf_idfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for first try\n",
    "top_terms = [\n",
    "    \"challenge\",\n",
    "    \"obama\",\n",
    "    \"romney\",\n",
    "    \"dangerous\",\n",
    "    \"administration\",\n",
    "    \"corruption\",\n",
    "    \"china\",\n",
    "    \"oil\",\n",
    "    \"iran\",\n",
    "]\n",
    "\n",
    "# for _last_specific approach\n",
    "top_terms = [\n",
    "    \"challenge\",\n",
    "    \"obama\",\n",
    "    \"romney\",\n",
    "    \"dangerous\",\n",
    "    \"administration\",\n",
    "    \"corruption\",\n",
    "    \"illegal\",\n",
    "    \"border\",\n",
    "    \"fail\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1\n",
    "\"\"\"\n",
    "top_terms = []\n",
    "for df in tf_idfs:\n",
    "    df = df.transpose()\n",
    "    df = df[0].sort_values(ascending=False)\n",
    "    top_terms.extend(df.index[:1])\n",
    "\n",
    "top_terms = list(dict.fromkeys(top_terms))\n",
    "\"\"\"\n",
    "preset_cooccurrences = []\n",
    "for i in top_terms:\n",
    "    for j in top_terms:\n",
    "        if i != j:\n",
    "            single_tuple = (i, j)\n",
    "            preset_cooccurrences.append(single_tuple)\n",
    "len(preset_cooccurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cooccurrences = []\n",
    "\n",
    "def hello_kitty_action_plan(model):\n",
    "    contingency_table = build_contingency_table_from_preset_cooccurrences(\n",
    "        RELATION_MODELS_PATH, model, preset_cooccurrences\n",
    "    )\n",
    "    return contingency_table\n",
    "\n",
    "\n",
    "# open multiprocessing pool\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "# multiprocessing\n",
    "raw_cooccurrences.append(pool.map(hello_kitty_action_plan, RELATION_MODELS))\n",
    "\n",
    "# close pool\n",
    "pool.close()\n",
    "\n",
    "raw_cooccurrences = raw_cooccurrences[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "# compare everything with everything\n",
    "for i in range(len(raw_cooccurrences)):\n",
    "    for j in range(len(raw_cooccurrences)):\n",
    "        if i != j:\n",
    "            contingency_table = pd.DataFrame()\n",
    "            df_results = pd.DataFrame()\n",
    "            description = (\n",
    "                raw_cooccurrences[i][0].index[0][-10:-7]\n",
    "                + \"-\"\n",
    "                + raw_cooccurrences[j][0].index[0][-10:-7]\n",
    "            )\n",
    "            contingency_table = pd.concat(\n",
    "                [raw_cooccurrences[i], raw_cooccurrences[j]], axis=0\n",
    "            )\n",
    "            contingency_table = sm.stats.Table(contingency_table)\n",
    "\n",
    "            # calculate results + add them to dataframe\n",
    "            results = contingency_table.test_nominal_association()\n",
    "\n",
    "            df_results[description] = [\n",
    "                raw_cooccurrences[i][0].index[0][-10:-7],\n",
    "                raw_cooccurrences[j][0].index[0][-10:-7],\n",
    "                description,\n",
    "                results.statistic,\n",
    "                results.pvalue,\n",
    "            ]\n",
    "\n",
    "            df_results = df_results.transpose()\n",
    "            df_results = df_results.rename(\n",
    "                columns={\n",
    "                    0: \"reference\",\n",
    "                    1: \"comparison\",\n",
    "                    2: \"combination\",\n",
    "                    3: str(\"chi_sq\"),\n",
    "                    4: str(\"p_value\"),\n",
    "                }\n",
    "            )\n",
    "            df = pd.concat([df, df_results], axis=0)\n",
    "\n",
    "\n",
    "# save results to csv\n",
    "df.to_csv(RESULTS_PATH + \"hyp1_\" + TOPIC_OF_INTEREST + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare everything with everything\n",
    "models = []\n",
    "\n",
    "# set up list of model combinations\n",
    "for i in RELATION_MODELS:\n",
    "    for j in RELATION_MODELS:\n",
    "        if i == j:\n",
    "            continue\n",
    "        else:\n",
    "            models.append([i, j])\n",
    "\n",
    "# initialize dataframes and counter for column name\n",
    "df = pd.DataFrame()\n",
    "\n",
    "def hello_kitty_action_plan(models):\n",
    "    # loop through model constellation in models list\n",
    "    df_results = pd.DataFrame()\n",
    "    description = str(str(models[0][-10:-7]) + \"-\" + str(models[1][-10:-7]))\n",
    "    try:\n",
    "        # create SQL query and build contingency table for sm.stats\n",
    "        contingency_table = build_contingency_table_from_single_topic(\n",
    "            RELATION_MODELS_PATH, models, TOPIC_OF_INTEREST, NO_ENTITIES\n",
    "        )\n",
    "        contingency_table = sm.stats.Table(contingency_table)\n",
    "        # calculate results + add them to dataframe\n",
    "        results = contingency_table.test_nominal_association()\n",
    "        df_results[description] = [str(models[0][-10:-7]), str(models[1][-10:-7]), description, results.statistic, results.pvalue]\n",
    "    except:\n",
    "        print(\"There is too little data for \" + TOPIC_OF_INTEREST)\n",
    "\n",
    "    df_results = df_results.transpose()\n",
    "    df_results = df_results.rename(columns={0: \"reference\", 1:\"comparison\", 2: \"combination\", 3: str(\"chi_sq\"), 4: str(\"p_value\")})\n",
    "\n",
    "    return df_results\n",
    "\n",
    "\n",
    "# Init multiprocessing.Pool()\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "# concat the pool-parallelized dataframes from pathlist\n",
    "df = pd.concat(pool.map(hello_kitty_action_plan, [model for model in models]), axis=0)\n",
    "\n",
    "# close pool \n",
    "pool.close()\n",
    "\n",
    "# save results to csv\n",
    "df.to_csv(\n",
    "    RESULTS_PATH + \"hyp1_\" + TOPIC_OF_INTEREST + \".csv\", index=False\n",
    ")\n",
    "\n",
    "\n",
    "message_me(\"hyp1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_OF_INTEREST = \"SPECIFIC_ENTITY_last_last_approach_Z\"\n",
    "# TOPIC_OF_INTEREST = \"obama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(RESULTS_PATH + \"hyp1_\" + TOPIC_OF_INTEREST + \".csv\")\n",
    "df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only necessary to run, if result csv is imported.\n",
    "# \"\"\"\n",
    "df[\"reference\"] = 1\n",
    "df[\"comparison\"] = 1\n",
    "df[\"ones\"] = 1\n",
    "for i in range(len(df)):\n",
    "    df.reference[i] = df.combination[i][:3]\n",
    "    df.comparison[i] = df.combination[i][-3:]\n",
    "# \"\"\"\n",
    "df[\"same_chi\"] = 1\n",
    "df[\"same_lit\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels for evaluating accuracy, precision and recall\n",
    "liberal = [\"NYT\", \"HFP\", \"WPO\"]\n",
    "center = [\"CNN\", \"RET\", \"NBC\"]\n",
    "conservative = [\"CTB\", \"FXN\", \"WSJ\"]\n",
    "\n",
    "\n",
    "for i in range(len(df)):\n",
    "    # adding labels, derived from the chi-test\n",
    "    if df.p_value[i] <= 0.05:\n",
    "        df.same_chi[i] = 1\n",
    "    else:\n",
    "        df.same_chi[i] = 0\n",
    "    \n",
    "    # adding labels, derived from literature\n",
    "    if all(x in liberal for x in [df.reference[i], df.comparison[i]]) == True:\n",
    "        df.same_lit[i] = 1\n",
    "    elif all(x in center for x in [df.reference[i], df.comparison[i]]) == True:\n",
    "        df.same_lit[i] = 1\n",
    "    elif all(x in conservative for x in [df.reference[i], df.comparison[i]]) == True:\n",
    "        df.same_lit[i] = 1\n",
    "    else:\n",
    "        df.same_lit[i] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results hypothesis 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into two dataframes df_same and df_diff\n",
    "df_same = pd.DataFrame()\n",
    "df_diff = pd.DataFrame()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if all(x in liberal for x in [df.reference[i], df.comparison[i]]) == True:\n",
    "        df_same = df_same.append(df.loc[i])\n",
    "    elif all(x in center for x in [df.reference[i], df.comparison[i]]) == True:\n",
    "        df_same = df_same.append(df.loc[i])\n",
    "    elif all(x in conservative for x in [df.reference[i], df.comparison[i]]) == True:\n",
    "        df_same = df_same.append(df.loc[i])\n",
    "    else:\n",
    "        df_diff = df_diff.append(df.loc[i])\n",
    "\n",
    "df_same = df_same.reset_index()\n",
    "df_diff = df_diff.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Results for Hyp 1.1\n",
    "y_true = df_same.same_lit\n",
    "y_pred = df_same.same_chi\n",
    "\n",
    "target_names = [\"diff\", \"same\"]\n",
    "print(classification_report(y_true, y_pred, target_names=target_names, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results hypothesis 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Results for Hyp 1.2\n",
    "y_true = df_diff.same_lit\n",
    "y_pred = df_diff.same_chi\n",
    "\n",
    "target_names = [\"diff\", \"same\"]\n",
    "print(classification_report(y_true, y_pred, target_names=target_names, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for overall precision\n",
    "y_true = df.same_lit\n",
    "y_pred = df.same_chi\n",
    "\n",
    "target_names = [\"diff\", \"same\"]\n",
    "print(classification_report(y_true, y_pred, target_names=target_names, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Hypothesis 2 <a class=\"anchor\" id=\"hyp2\"></a>\n",
    "\n",
    "Goal of hypothesis 2 estimates whether there are other possibilities to categorize news-outlets into similar slant groups. \n",
    "Therefore a graph is constructed from the news-outlets. The news-outlets are represented by nodes, the edges by the chi-value of the news-outlet tuples. The chi-statistic was chosen, since it respects differences in the number of articles of the compared news-outlets.\n",
    "\n",
    "The graphs were constructed as directed and undirected graphs.\n",
    "For each Version the local clustering coefficient of each node was calculated after Opsahl and Panzarasa (2009)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize as directed weighted graph\n",
    "G_weighted = nx.Graph()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    G_weighted.add_edge(df.reference[i], df.comparison[i], weight=(df.chi_sq[i] * 70))\n",
    "\n",
    "nx.spring_layout(G_weighted)\n",
    "plt.figure(figsize=(6, 6))\n",
    "nx.draw_networkx(G_weighted, with_labels=True, edge_color=\"gray\", node_color=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize as undirected weighted graph.\n",
    "# The edge, weights are combined and averaged \n",
    "\n",
    "df_new = pd.DataFrame()\n",
    "G_new = nx.Graph()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    k = pd.DataFrame()\n",
    "    for j in range(len(df)):\n",
    "        if (\n",
    "            df.reference[i] == df.comparison[j]\n",
    "            and df.reference[j] == df.comparison[i]\n",
    "            and i != j\n",
    "        ):\n",
    "            l = [\n",
    "                str(df.reference[i]),\n",
    "                str(df.comparison[i]),\n",
    "                ((df.chi_sq[i] + df.chi_sq[j])),\n",
    "            ]\n",
    "                      \n",
    "            G_new.add_edge(\n",
    "                str(df.reference[i]),\n",
    "                str(df.comparison[i]),\n",
    "                weight=(((df.chi_sq[i]+df.chi_sq[j])/2)*80),\n",
    "            )\n",
    "            \n",
    "            k = k.append(l)\n",
    "            k = k.transpose()\n",
    "            df_new = df_new.append(k)\n",
    "\n",
    "nx.spring_layout(G_new)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.axis = \"off\"\n",
    "nx.draw_networkx(G_new, with_labels=True, edge_color=\"gray\", node_color=\"white\")\n",
    "\n",
    "df_new = df_new.rename(columns={0: \"reference\", 1: \"comparison\", 2: \"summed_chi_sq\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the clustering coefficient for each node\n",
    "# normal cluster coefficient not useful since interconnected and weighted --> therefore Opsahl et al. (2009)\n",
    "\n",
    "# triplet values - arithmetic, geometric, max, min --> best results with geometric\n",
    "\n",
    "# split into list of dfs containing only one reference node\n",
    "df_list = [df.loc[i : i + 8 - 1, :] for i in range(0, len(df), 8)]\n",
    "\n",
    "df_coefficient = pd.DataFrame()\n",
    "\n",
    "# loop over every single node\n",
    "for df_single in df_list:\n",
    "    df_single = df_single.reset_index()\n",
    "    total_value = 0\n",
    "\n",
    "    # loop over the weights of all connected nodes\n",
    "    for j in range(len(df_single) - 1):\n",
    "        # geometric\n",
    "        # total_value = total_value + math.sqrt(           df_single.chi_sq[j] * df_single.chi_sq[j + 1]        )\n",
    "        # arithmetic\n",
    "        # total_value = total_value + (        (df_single.chi_sq[j] * df_single.chi_sq[j + 1]) / 2      )\n",
    "        # max\n",
    "        # total_value = total_value + max(df_single.chi_sq[j], df_single.chi_sq[j + 1])\n",
    "        # min\n",
    "        total_value = total_value + min(df_single.chi_sq[j], df_single.chi_sq[j + 1])\n",
    "\n",
    "    for i in range(len(df_single) - 1):\n",
    "        # geometric\n",
    "        # triplet_value = math.sqrt(df_single.chi_sq[i] * df_single.chi_sq[i + 1])\n",
    "        # arithmetic\n",
    "        # triplet_value = (df_single.chi_sq[i] * df_single.chi_sq[i + 1]) / 2\n",
    "        # max\n",
    "        # triplet_value = max(df_single.chi_sq[i], df_single.chi_sq[i + 1])\n",
    "        # min\n",
    "        triplet_value = min(df_single.chi_sq[i], df_single.chi_sq[i + 1])\n",
    "\n",
    "        cluster_coefficient = triplet_value / total_value\n",
    "        buffer = [\n",
    "            [\n",
    "                df_single.reference[i],\n",
    "                df_single.comparison[i],\n",
    "                df_single.comparison[i + 1],\n",
    "                triplet_value,\n",
    "                cluster_coefficient,\n",
    "            ]\n",
    "        ]\n",
    "        df_coefficient = df_coefficient.append(buffer)\n",
    "\n",
    "df_coefficient = df_coefficient.reset_index()\n",
    "\n",
    "\n",
    "check_list = []\n",
    "# print out triangles that have a cluster coefficient bigger, than X\n",
    "for i in range(len(df_coefficient)):\n",
    "    if df_coefficient[4][i] >= ((0.5) * df_coefficient[4].max()):\n",
    "        print(list(df_coefficient.loc[i][1:4]))\n",
    "        check_list.append(list(df_coefficient.loc[i][1:4]))\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether there is a correctly predicted triple\n",
    "liberal = [\"NYT\", \"HFP\", \"WPO\"]\n",
    "center = [\"CNN\", \"RET\", \"NBC\"]\n",
    "conservative = [\"CTB\", \"FXN\", \"WSJ\"]\n",
    "\n",
    "liberal.sort()\n",
    "center.sort()\n",
    "conservative.sort()\n",
    "\n",
    "# check_list.append([\"NYT\", \"HFP\", \"WPO\"])\n",
    "\n",
    "i = 0\n",
    "for element in check_list:\n",
    "    element.sort()\n",
    "    if element == liberal:\n",
    "        print(\"tatache - liberal \" + str(element))\n",
    "        i += 1\n",
    "    elif element == center:\n",
    "        print(\"tatsache - center \" + str(element))\n",
    "        i += 1\n",
    "    elif element == conservative:\n",
    "        print(\"tatsache - conservative \" + str(element))\n",
    "        i += 1\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "accuracy = i / (3 * 3 * 3)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
